{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fccb34",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.signal import butter, filtfilt, iirnotch, welch, detrend\n",
    "from sklearn.decomposition import FastICA\n",
    "from scipy.stats import skew, kurtosis\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# CONFIGURATION\n",
    "# ---------------------------------------\n",
    "channels = [\"Fp1\", \"Fp2\", \"F7\", \"F3\", \"FZ\", \"F4\", \"F8\", \"C2\"]\n",
    "sampling_rate = 250\n",
    "datasets = [\"Arithmetic_Data\", \"Stroop_Data\"]\n",
    "tasks = [\"natural\", \"lowlevel\", \"midlevel\", \"highlevel\"]\n",
    "num_subjects = 15\n",
    "\n",
    "window_size = 4              \n",
    "window_samples = window_size * sampling_rate\n",
    "overlap = 0.5\n",
    "step_size = int(window_samples * (1 - overlap))\n",
    "z_threshold = 3\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# ADVANCED PREPROCESSING\n",
    "# ---------------------------------------\n",
    "def bandpass_filter(signal, low=1, high=45, fs=250, order=4):\n",
    "    nyq = 0.5 * fs\n",
    "    b, a = butter(order, [low/nyq, high/nyq], btype=\"band\")\n",
    "    return filtfilt(b, a, signal)\n",
    "\n",
    "def notch_filter_50hz(signal, fs=250, Q=30):\n",
    "    nyq = 0.5 * fs\n",
    "    w0 = 50 / nyq\n",
    "    b, a = iirnotch(w0, Q)\n",
    "    return filtfilt(b, a, signal)\n",
    "\n",
    "def remove_outliers(signal, z_thresh=3):\n",
    "    z = (signal - np.mean(signal)) / (np.std(signal) + 1e-12)\n",
    "    signal[z > z_thresh] = np.nan\n",
    "    signal[z < -z_thresh] = np.nan\n",
    "\n",
    "    nans = np.isnan(signal)\n",
    "    if np.any(nans):\n",
    "        valid = np.flatnonzero(~nans)\n",
    "        signal[nans] = np.interp(np.flatnonzero(nans), valid, signal[valid])\n",
    "    return signal\n",
    "\n",
    "def baseline_drift_removal(signal):\n",
    "    return detrend(signal, type=\"linear\")\n",
    "\n",
    "def apply_ica_multi_channel(df):\n",
    "    try:\n",
    "        ica = FastICA(n_components=len(channels), random_state=42)\n",
    "        cleaned = ica.fit_transform(df.values)\n",
    "        return pd.DataFrame(cleaned, columns=channels)\n",
    "    except:\n",
    "        return df\n",
    "\n",
    "def preprocess_signal(signal):\n",
    "    signal = remove_outliers(signal)\n",
    "    signal = baseline_drift_removal(signal)\n",
    "    signal = bandpass_filter(signal, 1, 45, sampling_rate)\n",
    "    signal = notch_filter_50hz(signal, sampling_rate)\n",
    "    return signal\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# FREQUENCY BANDS\n",
    "# ---------------------------------------\n",
    "bands = {\n",
    "    \"delta\": (0.5, 4),\n",
    "    \"theta\": (4, 8),\n",
    "    \"alpha\": (8, 13),\n",
    "    \"beta\": (13, 30),\n",
    "    \"gamma\": (30, 40),\n",
    "    \"sigma\": (12, 16)\n",
    "}\n",
    "\n",
    "# ---------------------------------------\n",
    "# FEATURE EXTRACTION (UPDATED)\n",
    "# ---------------------------------------\n",
    "def extract_required_features(signal, fs=250):\n",
    "    feats = {}\n",
    "\n",
    "    # -------- BASIC TIME FEATURES --------\n",
    "    feats[\"mean\"] = np.mean(signal)\n",
    "    feats[\"median\"] = np.median(signal)\n",
    "    feats[\"variance\"] = np.var(signal)\n",
    "    feats[\"std\"] = np.std(signal)\n",
    "    feats[\"skewness\"] = skew(signal)\n",
    "    feats[\"kurtosis\"] = kurtosis(signal)\n",
    "\n",
    "    # -------- POWER SPECTRAL FEATURES --------\n",
    "    f, Pxx = welch(signal, fs=fs, nperseg=fs)\n",
    "    band_powers = {}\n",
    "\n",
    "    for band, (low, high) in bands.items():\n",
    "        mask = (f >= low) & (f < high)\n",
    "        power = np.sum(Pxx[mask])\n",
    "        feats[f\"{band}_power\"] = power\n",
    "        band_powers[band] = power\n",
    "\n",
    "    # avoid zero division\n",
    "    d = band_powers[\"delta\"] + 1e-8\n",
    "    t = band_powers[\"theta\"] + 1e-8\n",
    "    a = band_powers[\"alpha\"] + 1e-8\n",
    "    b = band_powers[\"beta\"] + 1e-8\n",
    "    g = band_powers[\"gamma\"] + 1e-8\n",
    "    s = band_powers[\"sigma\"] + 1e-8\n",
    "\n",
    "    # -------- REQUIRED RATIOS FROM BOTH IMAGES --------\n",
    "    # (Group 1)\n",
    "    feats[\"theta_alpha_ratio\"] = t / a\n",
    "    feats[\"beta_alpha_ratio\"] = b / a\n",
    "    feats[\"theta_plus_alpha_over_beta\"] = (t + a) / b\n",
    "\n",
    "    # (Group 2 - More ratios)\n",
    "    feats[\"theta_beta_ratio\"] = t / b\n",
    "    feats[\"theta_plus_alpha_over_alpha_plus_beta\"] = (t + a) / (a + b)\n",
    "    feats[\"gamma_delta_ratio\"] = g / d\n",
    "    feats[\"gamma_plus_beta_over_delta_plus_alpha\"] = (g + b) / (d + a)\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# SEGMENTATION\n",
    "# ---------------------------------------\n",
    "def segment_signal(signal):\n",
    "    return [\n",
    "        signal[i:i + window_samples]\n",
    "        for i in range(0, len(signal) - window_samples + 1, step_size)\n",
    "    ]\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# MAIN EXTRACTION LOOP\n",
    "# ---------------------------------------\n",
    "def build_feature_dataset(base_path):\n",
    "    all_rows = []\n",
    "\n",
    "    for dataset in datasets:\n",
    "        print(\"\\nProcessing dataset:\", dataset)\n",
    "        folder = os.path.join(base_path, dataset)\n",
    "\n",
    "        for subject in range(1, num_subjects + 1):\n",
    "\n",
    "            subject_data = {}\n",
    "            lengths = {}\n",
    "\n",
    "            for task in tasks:\n",
    "                file_path = os.path.join(folder, f\"{task}-{subject}.txt\")\n",
    "                if not os.path.exists(file_path):\n",
    "                    continue\n",
    "\n",
    "                df = pd.read_csv(file_path, header=None).iloc[:, 1:1+len(channels)]\n",
    "                df.columns = channels\n",
    "\n",
    "                for ch in channels:\n",
    "                    df[ch] = df[ch].astype(str).str.rstrip(',').astype(float)\n",
    "\n",
    "                subject_data[task] = df\n",
    "                lengths[task] = len(df)\n",
    "\n",
    "            if len(subject_data) < len(tasks):\n",
    "                continue\n",
    "\n",
    "            min_len = min(lengths.values())\n",
    "            trim_start = 50\n",
    "            trim_end = min_len - 50\n",
    "\n",
    "            print(f\"  Subject {subject}: trimmed length = {trim_end - trim_start}\")\n",
    "\n",
    "            for task in tasks:\n",
    "\n",
    "                df = subject_data[task].iloc[trim_start:trim_end].reset_index(drop=True)\n",
    "\n",
    "                df = apply_ica_multi_channel(df)\n",
    "\n",
    "                for ch in channels:\n",
    "                    df[ch] = preprocess_signal(df[ch].values)\n",
    "\n",
    "                for start in range(0, len(df) - window_samples, step_size):\n",
    "                    row = {}\n",
    "\n",
    "                    for ch in channels:\n",
    "                        seg = df[ch].values[start:start + window_samples]\n",
    "                        feats = extract_required_features(seg)\n",
    "                        for k, v in feats.items():\n",
    "                            row[f\"{ch}_{k}\"] = v\n",
    "\n",
    "                    row[\"subject\"] = subject\n",
    "                    row[\"task\"] = task\n",
    "                    row[\"dataset\"] = dataset\n",
    "                    row[\"window_id\"] = start\n",
    "\n",
    "                    all_rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(all_rows)\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# RUN PIPELINE\n",
    "# ---------------------------------------\n",
    "base_path = \"/kaggle/input/tempora/Cognitive Load Assessment Through EEG A Dataset from Arithmetic and Stroop Tasks\"\n",
    "features_df = build_feature_dataset(base_path)\n",
    "\n",
    "features_df.to_csv(\"frequency_features_extended.csv\", index=False)\n",
    "\n",
    "print(\"\\nâœ… Saved: frequency_features_extended.csv (ALL ratios included!)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dbab8d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# 1. IMPORTS\n",
    "# ===========================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Models\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# 2. LOAD DATA\n",
    "# ===========================\n",
    "df = pd.read_csv(\"frequency_features_extended.csv\")\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# 3. CLEAN\n",
    "# ===========================\n",
    "df[\"dataset\"] = df[\"dataset\"].astype(str).str.strip().str.lower()\n",
    "df[\"task\"] = df[\"task\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# 4. FILTER DESIRED DATA\n",
    "# Keep all 4 levels: natural, lowlevel, midlevel, highlevel\n",
    "# ===========================\n",
    "df = df[\n",
    "    (df[\"dataset\"] == \"stroop_data\") &\n",
    "    (df[\"task\"].isin([\"natural\", \"lowlevel\", \"midlevel\", \"highlevel\"]))\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# FOUR-CLASS label mapping\n",
    "label_map = {\n",
    "    \"natural\": 0,\n",
    "    \"lowlevel\": 1,\n",
    "    \"midlevel\": 2,\n",
    "    \"highlevel\": 3\n",
    "}\n",
    "\n",
    "df[\"task_label\"] = df[\"task\"].map(label_map)\n",
    "\n",
    "# Drop unused columns\n",
    "df = df.drop(columns=[\"dataset\", \"task\", \"window_id\"], errors=\"ignore\")\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# 5. SPLIT FEATURES / TARGET\n",
    "# ===========================\n",
    "X = df.drop(columns=[\"task_label\"])\n",
    "y = df[\"task_label\"]\n",
    "\n",
    "# Scale inputs\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# 6. DEFINE MULTI-CLASS MODELS (4 classes)\n",
    "# ===========================\n",
    "models = {\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        objective=\"multi:softprob\",\n",
    "        num_class=4,\n",
    "        eval_metric=\"mlogloss\",\n",
    "        n_estimators=400,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=8,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "    ),\n",
    "\n",
    "    \"Random Forest\": RandomForestClassifier(\n",
    "        n_estimators=400, random_state=42\n",
    "    ),\n",
    "\n",
    "    \"Logistic Regression\": LogisticRegression(\n",
    "        max_iter=3000, solver=\"lbfgs\", multi_class=\"multinomial\"\n",
    "    ),\n",
    "\n",
    "    \"SVM (RBF)\": SVC(\n",
    "        kernel=\"rbf\", probability=True, C=3, gamma=\"scale\"\n",
    "    ),\n",
    "\n",
    "    \"KNN\": KNeighborsClassifier(\n",
    "        n_neighbors=7\n",
    "    ),\n",
    "\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(\n",
    "        n_estimators=300, learning_rate=0.05, random_state=42\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# 7. TRAIN + EVALUATE MODELS\n",
    "# ===========================\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average=\"weighted\")   # weighted for multi-class\n",
    "\n",
    "    results.append((name, acc, f1))\n",
    "\n",
    "    print(\"\\n===========================\")\n",
    "    print(f\"ðŸ“Œ MODEL: {name}\")\n",
    "    print(\"===========================\")\n",
    "    print(f\"Accuracy = {acc:.4f}\")\n",
    "    print(f\"F1 Score = {f1:.4f}\")\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# 8. MODEL COMPARISON SUMMARY\n",
    "# ===========================\n",
    "print(\"\\n===========================\")\n",
    "print(\"ðŸ“Š MODEL COMPARISON SUMMARY (4-CLASS)\")\n",
    "print(\"===========================\")\n",
    "\n",
    "df_results = pd.DataFrame(results, columns=[\"Model\", \"Accuracy\", \"F1 Score\"])\n",
    "print(df_results.sort_values(\"Accuracy\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44dfbea",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ===========================\n",
    "# 1. LOAD DATA\n",
    "# ===========================\n",
    "df = pd.read_csv(\"frequency_features_extended.csv\")\n",
    "\n",
    "# ===========================\n",
    "# 2. CLEAN\n",
    "# ===========================\n",
    "df[\"dataset\"] = df[\"dataset\"].astype(str).str.lower().str.strip()\n",
    "df[\"task\"] = df[\"task\"].astype(str).str.lower().str.strip()\n",
    "\n",
    "# ===========================\n",
    "# 3. TASK LABEL: Arithmetic vs Stroop\n",
    "# ===========================\n",
    "df[\"task_type\"] = df[\"dataset\"].map(lambda x: 0 if x == \"arithmetic_data\" else 1)\n",
    "\n",
    "# Remove columns not needed\n",
    "df_task = df.drop(columns=[\"task\", \"window_id\"], errors=\"ignore\")\n",
    "\n",
    "# ===========================\n",
    "# 4. SPLIT\n",
    "# ===========================\n",
    "X_task = df_task.drop(columns=[\"dataset\", \"task_type\"])\n",
    "y_task = df_task[\"task_type\"]\n",
    "\n",
    "scaler_task = StandardScaler()\n",
    "X_task_scaled = scaler_task.fit_transform(X_task)\n",
    "\n",
    "X_train_t, X_test_t, y_train_t, y_test_t = train_test_split(\n",
    "    X_task_scaled, y_task, test_size=0.2, random_state=42, stratify=y_task\n",
    ")\n",
    "\n",
    "# ===========================\n",
    "# 5. MODELS\n",
    "# ===========================\n",
    "models_task = {\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"logloss\",\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "    ),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=300, random_state=42),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=2000),\n",
    "    \"SVM (RBF)\": SVC(kernel=\"rbf\", probability=True),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=7),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=300),\n",
    "}\n",
    "\n",
    "# ===========================\n",
    "# 6. TRAIN + EVALUATE\n",
    "# ===========================\n",
    "print(\"\\n==============================\")\n",
    "print(\"ðŸ”µ TASK CLASSIFICATION RESULTS\")\n",
    "print(\"==============================\")\n",
    "\n",
    "task_results = []\n",
    "\n",
    "for name, model in models_task.items():\n",
    "\n",
    "    model.fit(X_train_t, y_train_t)\n",
    "    y_pred_t = model.predict(X_test_t)\n",
    "\n",
    "    acc = accuracy_score(y_test_t, y_pred_t)\n",
    "    f1 = f1_score(y_test_t, y_pred_t)\n",
    "\n",
    "    task_results.append((name, acc, f1))\n",
    "\n",
    "    print(f\"\\nðŸ“Œ Model: {name}\")\n",
    "    print(f\"Accuracy = {acc:.4f}\")\n",
    "    print(f\"F1 Score = {f1:.4f}\")\n",
    "\n",
    "df_task_results = pd.DataFrame(task_results, columns=[\"Model\", \"Accuracy\", \"F1 Score\"])\n",
    "print(\"\\nTask Classification Summary:\")\n",
    "print(df_task_results.sort_values(\"Accuracy\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b343b0ec",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.signal import butter, filtfilt, iirnotch, welch, detrend, hilbert\n",
    "from sklearn.decomposition import FastICA\n",
    "from scipy.stats import skew, kurtosis\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# CONFIGURATION\n",
    "# ---------------------------------------\n",
    "channels = [\"Fp1\", \"Fp2\", \"F7\", \"F3\", \"FZ\", \"F4\", \"F8\", \"C2\"]\n",
    "sampling_rate = 250\n",
    "datasets = [\"Arithmetic_Data\", \"Stroop_Data\"]\n",
    "tasks = [\"natural\", \"lowlevel\", \"midlevel\", \"highlevel\"]\n",
    "num_subjects = 15\n",
    "\n",
    "window_size = 4              \n",
    "window_samples = window_size * sampling_rate\n",
    "overlap = 0.5\n",
    "step_size = int(window_samples * (1 - overlap))\n",
    "z_threshold = 3\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# ADVANCED PREPROCESSING\n",
    "# ---------------------------------------\n",
    "def bandpass_filter(signal, low=1, high=45, fs=250, order=4):\n",
    "    nyq = 0.5 * fs\n",
    "    b, a = butter(order, [low/nyq, high/nyq], btype=\"band\")\n",
    "    return filtfilt(b, a, signal)\n",
    "\n",
    "def notch_filter_50hz(signal, fs=250, Q=30):\n",
    "    nyq = 0.5 * fs\n",
    "    w0 = 50 / nyq\n",
    "    b, a = iirnotch(w0, Q)\n",
    "    return filtfilt(b, a, signal)\n",
    "\n",
    "def remove_outliers(signal, z_thresh=3):\n",
    "    z = (signal - np.mean(signal)) / (np.std(signal) + 1e-12)\n",
    "    signal[z > z_thresh] = np.nan\n",
    "    signal[z < -z_thresh] = np.nan\n",
    "\n",
    "    # interpolate nans\n",
    "    nans = np.isnan(signal)\n",
    "    if np.any(nans):\n",
    "        valid = np.flatnonzero(~nans)\n",
    "        signal[nans] = np.interp(np.flatnonzero(nans), valid, signal[valid])\n",
    "    return signal\n",
    "\n",
    "def baseline_drift_removal(signal):\n",
    "    return detrend(signal, type=\"linear\")\n",
    "\n",
    "def apply_ica_multi_channel(df):\n",
    "    try:\n",
    "        ica = FastICA(n_components=len(channels), random_state=42)\n",
    "        cleaned = ica.fit_transform(df.values)\n",
    "        return pd.DataFrame(cleaned, columns=channels)\n",
    "    except:\n",
    "        return df\n",
    "\n",
    "\n",
    "def preprocess_signal(signal):\n",
    "    signal = remove_outliers(signal)\n",
    "    signal = baseline_drift_removal(signal)\n",
    "    signal = bandpass_filter(signal, 1, 45, sampling_rate)\n",
    "    signal = notch_filter_50hz(signal, sampling_rate)\n",
    "    return signal\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# TIME DOMAIN FEATURE EXTRACTION\n",
    "# ---------------------------------------\n",
    "\n",
    "# Hjorth parameters\n",
    "def hjorth_params(x):\n",
    "    dx = np.diff(x)\n",
    "    ddx = np.diff(dx)\n",
    "\n",
    "    var_x = np.var(x)\n",
    "    var_dx = np.var(dx)\n",
    "    var_ddx = np.var(ddx)\n",
    "\n",
    "    activity = var_x\n",
    "    mobility = np.sqrt(var_dx / var_x) if var_x != 0 else 0\n",
    "    complexity = np.sqrt(var_ddx / var_dx) / mobility if var_dx != 0 else 0\n",
    "\n",
    "    return activity, mobility, complexity\n",
    "\n",
    "# Instantaneous frequency\n",
    "def instantaneous_frequency(signal, fs):\n",
    "    analytic = hilbert(signal)\n",
    "    phase = np.unwrap(np.angle(analytic))\n",
    "    inst_freq = np.diff(phase) * (fs / (2 * np.pi))\n",
    "    return np.mean(inst_freq) if len(inst_freq) > 1 else 0\n",
    "\n",
    "\n",
    "def extract_time_features(signal, fs=250):\n",
    "\n",
    "    activity, mobility, complexity = hjorth_params(signal)\n",
    "    inst_freq = instantaneous_frequency(signal, fs)\n",
    "\n",
    "    feats = {\n",
    "        \"mean\": np.mean(signal),\n",
    "        \"median\": np.median(signal),\n",
    "        \"variance\": np.var(signal),\n",
    "        \"std\": np.std(signal),\n",
    "        \"skewness\": skew(signal),\n",
    "        \"kurtosis\": kurtosis(signal),\n",
    "        \"hjorth_activity\": activity,\n",
    "        \"hjorth_mobility\": mobility,\n",
    "        \"hjorth_complexity\": complexity,\n",
    "        \"instantaneous_frequency\": inst_freq,\n",
    "    }\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# SEGMENTATION\n",
    "# ---------------------------------------\n",
    "def segment_signal(signal):\n",
    "    return [\n",
    "        signal[i:i + window_samples]\n",
    "        for i in range(0, len(signal) - window_samples + 1, step_size)\n",
    "    ]\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# MAIN EXTRACTION LOOP\n",
    "# ---------------------------------------\n",
    "def build_feature_dataset(base_path):\n",
    "    all_rows = []\n",
    "\n",
    "    for dataset in datasets:\n",
    "        print(\"\\nProcessing dataset:\", dataset)\n",
    "        folder = os.path.join(base_path, dataset)\n",
    "\n",
    "        for subject in range(1, num_subjects + 1):\n",
    "\n",
    "            subject_data = {}\n",
    "            lengths = {}\n",
    "\n",
    "            for task in tasks:\n",
    "                file_path = os.path.join(folder, f\"{task}-{subject}.txt\")\n",
    "                if not os.path.exists(file_path):\n",
    "                    continue\n",
    "\n",
    "                df = pd.read_csv(file_path, header=None).iloc[:, 1:1+len(channels)]\n",
    "                df.columns = channels\n",
    "\n",
    "                for ch in channels:\n",
    "                    df[ch] = df[ch].astype(str).str.rstrip(',').astype(float)\n",
    "\n",
    "                subject_data[task] = df\n",
    "                lengths[task] = len(df)\n",
    "\n",
    "            if len(subject_data) < len(tasks):\n",
    "                continue\n",
    "\n",
    "            min_len = min(lengths.values())\n",
    "            trim_start = 50\n",
    "            trim_end = min_len - 50\n",
    "\n",
    "            print(f\"  Subject {subject}: trimmed length = {trim_end - trim_start}\")\n",
    "\n",
    "            for task in tasks:\n",
    "\n",
    "                df = subject_data[task].iloc[trim_start:trim_end].reset_index(drop=True)\n",
    "\n",
    "                df = apply_ica_multi_channel(df)\n",
    "\n",
    "                for ch in channels:\n",
    "                    df[ch] = preprocess_signal(df[ch].values)\n",
    "\n",
    "                for start in range(0, len(df) - window_samples, step_size):\n",
    "                    row = {}\n",
    "\n",
    "                    for ch in channels:\n",
    "                        seg = df[ch].values[start:start + window_samples]\n",
    "                        feats = extract_time_features(seg)\n",
    "\n",
    "                        for k, v in feats.items():\n",
    "                            row[f\"{ch}_{k}\"] = v\n",
    "\n",
    "                    row[\"subject\"] = subject\n",
    "                    row[\"task\"] = task\n",
    "                    row[\"dataset\"] = dataset\n",
    "                    row[\"window_id\"] = start\n",
    "\n",
    "                    all_rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(all_rows)\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# RUN PIPELINE\n",
    "# ---------------------------------------\n",
    "base_path = \"/kaggle/input/tempora/Cognitive Load Assessment Through EEG A Dataset from Arithmetic and Stroop Tasks\"\n",
    "features_df = build_feature_dataset(base_path)\n",
    "\n",
    "features_df.to_csv(\"time_domain_features_updated.csv\", index=False)\n",
    "\n",
    "print(\"\\nâœ… Saved: time_domain_features_updated.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf7c4a6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ===========================\n",
    "# 1. LOAD TIME-DOMAIN DATA\n",
    "# ===========================\n",
    "df = pd.read_csv(\"time_domain_features_updated.csv\")   # UPDATED\n",
    "\n",
    "# ===========================\n",
    "# 2. CLEAN\n",
    "# ===========================\n",
    "df[\"dataset\"] = df[\"dataset\"].astype(str).str.lower().str.strip()\n",
    "df[\"task\"] = df[\"task\"].astype(str).str.lower().str.strip()\n",
    "\n",
    "# ===========================\n",
    "# 3. TASK LABEL: Arithmetic vs Stroop\n",
    "# ===========================\n",
    "df[\"task_type\"] = df[\"dataset\"].map(lambda x: 0 if x == \"arithmetic_data\" else 1)\n",
    "\n",
    "# ===========================\n",
    "# 4. REMOVE NON-FEATURE COLUMNS\n",
    "# ===========================\n",
    "df_task = df.drop(columns=[\"task\", \"window_id\"], errors=\"ignore\")\n",
    "\n",
    "# features = all columns except dataset + task_type\n",
    "X_task = df_task.drop(columns=[\"dataset\", \"task_type\"], errors=\"ignore\")\n",
    "y_task = df_task[\"task_type\"]\n",
    "\n",
    "# ===========================\n",
    "# 5. SCALE FEATURES\n",
    "# ===========================\n",
    "scaler_task = StandardScaler()\n",
    "X_task_scaled = scaler_task.fit_transform(X_task)\n",
    "\n",
    "# ===========================\n",
    "# 6. TRAIN/TEST SPLIT\n",
    "# ===========================\n",
    "X_train_t, X_test_t, y_train_t, y_test_t = train_test_split(\n",
    "    X_task_scaled, y_task, test_size=0.2, random_state=42, stratify=y_task\n",
    ")\n",
    "\n",
    "# ===========================\n",
    "# 7. MODELS\n",
    "# ===========================\n",
    "models_task = {\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"logloss\",\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "    ),\n",
    "\n",
    "    \"Random Forest\": RandomForestClassifier(\n",
    "        n_estimators=300, random_state=42\n",
    "    ),\n",
    "\n",
    "    \"Logistic Regression\": LogisticRegression(\n",
    "        max_iter=3000\n",
    "    ),\n",
    "\n",
    "    \"SVM (RBF)\": SVC(\n",
    "        kernel=\"rbf\", probability=True, C=2\n",
    "    ),\n",
    "\n",
    "    \"KNN\": KNeighborsClassifier(\n",
    "        n_neighbors=7\n",
    "    ),\n",
    "\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(\n",
    "        n_estimators=300, learning_rate=0.05, random_state=42\n",
    "    ),\n",
    "}\n",
    "\n",
    "# ===========================\n",
    "# 8. TRAIN + EVALUATE MODELS\n",
    "# ===========================\n",
    "print(\"\\n==============================\")\n",
    "print(\"ðŸ”µ TASK CLASSIFICATION RESULTS (Time-Domain Features)\")\n",
    "print(\"==============================\")\n",
    "\n",
    "task_results = []\n",
    "\n",
    "for name, model in models_task.items():\n",
    "\n",
    "    model.fit(X_train_t, y_train_t)\n",
    "    y_pred_t = model.predict(X_test_t)\n",
    "\n",
    "    acc = accuracy_score(y_test_t, y_pred_t)\n",
    "    f1 = f1_score(y_test_t, y_pred_t)\n",
    "\n",
    "    task_results.append((name, acc, f1))\n",
    "\n",
    "    print(f\"\\nðŸ“Œ Model: {name}\")\n",
    "    print(f\"Accuracy = {acc:.4f}\")\n",
    "    print(f\"F1 Score = {f1:.4f}\")\n",
    "\n",
    "df_task_results = pd.DataFrame(task_results, columns=[\"Model\", \"Accuracy\", \"F1 Score\"])\n",
    "\n",
    "print(\"\\nTask Classification Summary:\")\n",
    "print(df_task_results.sort_values(\"Accuracy\", ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03425faa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# 1. IMPORTS\n",
    "# ===========================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Models\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# 2. LOAD TIME-DOMAIN DATA\n",
    "# ===========================\n",
    "df = pd.read_csv(\"time_domain_features_updated.csv\")   # UPDATED FILE\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# 3. CLEAN STRINGS\n",
    "# ===========================\n",
    "df[\"dataset\"] = df[\"dataset\"].astype(str).str.strip().str.lower()\n",
    "df[\"task\"] = df[\"task\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# 4. FILTER FOR STROOP 4-LEVEL TASKS\n",
    "# ===========================\n",
    "df = df[\n",
    "    (df[\"dataset\"] == \"stroop_data\") &\n",
    "    (df[\"task\"].isin([\"natural\", \"lowlevel\", \"midlevel\", \"highlevel\"]))\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# 4-class mapping\n",
    "label_map = {\n",
    "    \"natural\": 0,\n",
    "    \"lowlevel\": 1,\n",
    "    \"midlevel\": 2,\n",
    "    \"highlevel\": 3\n",
    "}\n",
    "\n",
    "df[\"task_label\"] = df[\"task\"].map(label_map)\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# 5. DROP UNUSED COLUMNS\n",
    "# ===========================\n",
    "df = df.drop(columns=[\"dataset\", \"task\", \"window_id\"], errors=\"ignore\")\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# 6. SPLIT FEATURES / TARGET\n",
    "# ===========================\n",
    "X = df.drop(columns=[\"task_label\"])   # all time-domain feature columns\n",
    "y = df[\"task_label\"]\n",
    "\n",
    "# Standard scaling (required for SVM, KNN, Logistic Regression)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# 7. DEFINE MULTI-CLASS MODELS\n",
    "# ===========================\n",
    "models = {\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        objective=\"multi:softprob\",\n",
    "        num_class=4,\n",
    "        eval_metric=\"mlogloss\",\n",
    "        n_estimators=400,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=8,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "    ),\n",
    "\n",
    "    \"Random Forest\": RandomForestClassifier(\n",
    "        n_estimators=400, random_state=42\n",
    "    ),\n",
    "\n",
    "    \"Logistic Regression\": LogisticRegression(\n",
    "        max_iter=3000, solver=\"lbfgs\", multi_class=\"multinomial\"\n",
    "    ),\n",
    "\n",
    "    \"SVM (RBF)\": SVC(\n",
    "        kernel=\"rbf\", probability=True, C=3\n",
    "    ),\n",
    "\n",
    "    \"KNN\": KNeighborsClassifier(\n",
    "        n_neighbors=7\n",
    "    ),\n",
    "\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(\n",
    "        n_estimators=300, learning_rate=0.05, random_state=42\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# 8. TRAIN + EVALUATE MODELS\n",
    "# ===========================\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "\n",
    "    results.append((name, acc, f1))\n",
    "\n",
    "    print(\"\\n===========================\")\n",
    "    print(f\"ðŸ“Œ MODEL: {name}\")\n",
    "    print(\"===========================\")\n",
    "    print(f\"Accuracy = {acc:.4f}\")\n",
    "    print(f\"F1 Score = {f1:.4f}\")\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# 9. COMPARISON SUMMARY\n",
    "# ===========================\n",
    "print(\"\\n===========================\")\n",
    "print(\"ðŸ“Š MODEL COMPARISON SUMMARY (4-CLASS, TIME DOMAIN)\")\n",
    "print(\"===========================\")\n",
    "\n",
    "df_results = pd.DataFrame(results, columns=[\"Model\", \"Accuracy\", \"F1 Score\"])\n",
    "print(df_results.sort_values(\"Accuracy\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b97cdb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from scipy.signal import butter, filtfilt, iirnotch, detrend, welch, hilbert\n",
    "from sklearn.decomposition import FastICA\n",
    "from scipy.stats import skew, kurtosis\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"seaborn\")\n",
    "\n",
    "# ====================================================================\n",
    "# CONFIGURATION\n",
    "# ====================================================================\n",
    "channels = [\"Fp1\", \"Fp2\", \"F7\", \"F3\", \"FZ\", \"F4\", \"F8\", \"C2\"]\n",
    "sampling_rate = 250\n",
    "datasets = [\"Arithmetic_Data\", \"Stroop_Data\"]\n",
    "tasks = [\"natural\", \"lowlevel\", \"midlevel\", \"highlevel\"]\n",
    "num_subjects = 15\n",
    "window_size = 4\n",
    "window_samples = sampling_rate * window_size\n",
    "step_size = window_samples // 2\n",
    "\n",
    "# ====================================================================\n",
    "# PLOTTING (ONLY FIRST CHANNEL OF FIRST SUBJECT+TASK)\n",
    "# ====================================================================\n",
    "PLOT_ENABLED = True\n",
    "PLOT_DONE = False\n",
    "\n",
    "def safe_plot(func, *args, **kwargs):\n",
    "    global PLOT_DONE\n",
    "    if PLOT_ENABLED and not PLOT_DONE:\n",
    "        func(*args, **kwargs)\n",
    "\n",
    "# ====================================================================\n",
    "# PREPROCESSING FUNCTIONS\n",
    "# ====================================================================\n",
    "def bandpass_filter(sig, low=1, high=45, fs=250, order=4):\n",
    "    nyq = 0.5 * fs\n",
    "    b, a = butter(order, [low / nyq, high / nyq], btype=\"band\")\n",
    "    return filtfilt(b, a, sig)\n",
    "\n",
    "def notch_filter(sig, fs=250, Q=30):\n",
    "    nyq = 0.5 * fs\n",
    "    b, a = iirnotch(50 / nyq, Q)\n",
    "    return filtfilt(b, a, sig)\n",
    "\n",
    "def remove_outliers(sig):\n",
    "    z = (sig - np.mean(sig)) / (np.std(sig) + 1e-6)\n",
    "    sig[z > 3] = np.nan\n",
    "    sig[z < -3] = np.nan\n",
    "    nans = np.isnan(sig)\n",
    "    if np.any(nans):\n",
    "        valid = np.flatnonzero(~nans)\n",
    "        sig[nans] = np.interp(np.flatnonzero(nans), valid, sig[valid])\n",
    "    return sig\n",
    "\n",
    "# ====================================================================\n",
    "# FEATURE EXTRACTION\n",
    "# ====================================================================\n",
    "def hjorth_params(x):\n",
    "    dx = np.diff(x)\n",
    "    ddx = np.diff(dx)\n",
    "    var0 = np.var(x)\n",
    "    var1 = np.var(dx)\n",
    "    var2 = np.var(ddx)\n",
    "    activity = var0\n",
    "    mobility = np.sqrt(var1 / var0) if var0 else 0\n",
    "    complexity = np.sqrt(var2 / var1) / mobility if var1 else 0\n",
    "    return activity, mobility, complexity\n",
    "\n",
    "def inst_freq(sig, fs):\n",
    "    analytic = hilbert(sig)\n",
    "    phase = np.unwrap(np.angle(analytic))\n",
    "    inst = np.diff(phase) * fs / (2*np.pi)\n",
    "    return np.mean(inst) if len(inst) else 0\n",
    "\n",
    "def extract_features(seg):\n",
    "    A, M, C = hjorth_params(seg)\n",
    "    IF = inst_freq(seg, sampling_rate)\n",
    "    return {\n",
    "        \"mean\": np.mean(seg),\n",
    "        \"median\": np.median(seg),\n",
    "        \"variance\": np.var(seg),\n",
    "        \"std\": np.std(seg),\n",
    "        \"skewness\": skew(seg),\n",
    "        \"kurtosis\": kurtosis(seg),\n",
    "        \"hjorth_activity\": A,\n",
    "        \"hjorth_mobility\": M,\n",
    "        \"hjorth_complexity\": C,\n",
    "        \"instantaneous_frequency\": IF\n",
    "    }\n",
    "\n",
    "# ====================================================================\n",
    "# SEGMENTATION\n",
    "# ====================================================================\n",
    "def segment(sig):\n",
    "    return [\n",
    "        sig[i:i + window_samples]\n",
    "        for i in range(0, len(sig) - window_samples, step_size)\n",
    "    ]\n",
    "\n",
    "# ====================================================================\n",
    "# MAIN PIPELINE\n",
    "# ====================================================================\n",
    "def build_features(base_path):\n",
    "    global PLOT_DONE\n",
    "    all_features = []\n",
    "\n",
    "    for dataset in datasets:\n",
    "        for subject in range(1, num_subjects + 1):\n",
    "\n",
    "            for task in tasks:\n",
    "                fpath = os.path.join(base_path, dataset, f\"{task}-{subject}.txt\")\n",
    "                if not os.path.exists(fpath):\n",
    "                    continue\n",
    "\n",
    "                # Load EEG\n",
    "                df = pd.read_csv(fpath, header=None).iloc[:, 1:9]\n",
    "                df.columns = channels\n",
    "\n",
    "                # Clean format\n",
    "                for ch in channels:\n",
    "                    df[ch] = df[ch].astype(str).str.rstrip(\",\").astype(float)\n",
    "\n",
    "                # ICA once per task (fast)\n",
    "                try:\n",
    "                    ica = FastICA(n_components=len(channels), random_state=42)\n",
    "                    df = pd.DataFrame(ica.fit_transform(df), columns=channels)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                for ch in channels:\n",
    "                    sig = df[ch].values.copy()\n",
    "\n",
    "                    # Only plot ONCE\n",
    "                    if not PLOT_DONE:\n",
    "                        t = np.arange(len(sig)) / sampling_rate\n",
    "                        plt.plot(t, sig); plt.title(f\"Raw Signal: {ch}\"); plt.show()\n",
    "\n",
    "                    sig1 = remove_outliers(sig)\n",
    "                    safe_plot(lambda b,a: plt.plot(b); plt.plot(a); plt.title(\"Outlier removal\"), sig, sig1)\n",
    "\n",
    "                    sig2 = detrend(sig1)\n",
    "                    sig3 = bandpass_filter(sig2)\n",
    "                    sig4 = notch_filter(sig3)\n",
    "\n",
    "                    df[ch] = sig4\n",
    "\n",
    "                PLOT_DONE = True  # Stop plotting for rest of pipeline\n",
    "\n",
    "                # Segmentation + Feature extraction\n",
    "                for ch in channels:\n",
    "                    segments = segment(df[ch].values)\n",
    "                    for seg in segments:\n",
    "                        feats = extract_features(seg)\n",
    "                        feats[\"dataset\"] = dataset\n",
    "                        feats[\"task\"] = task\n",
    "                        feats[\"subject\"] = subject\n",
    "                        feats[\"channel\"] = ch\n",
    "                        all_features.append(feats)\n",
    "\n",
    "    return pd.DataFrame(all_features)\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# RUN PIPELINE FAST\n",
    "# ====================================================================\n",
    "base = \"/kaggle/input/tempora/Cognitive Load Assessment Through EEG A Dataset from Arithmetic and Stroop Tasks\"\n",
    "\n",
    "features = build_features(base)\n",
    "features.to_csv(\"time_features_final.csv\", index=False)\n",
    "print(\"âœ” FAST PIPELINE COMPLETE\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
